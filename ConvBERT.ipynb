{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ConvBERT.ipynb","provenance":[],"collapsed_sections":["BFd7oY1CRzGV","_fihoZNkAA7u","QcblYjq9b_PV","mOu00nOEM_gL"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TnWYh-GKpH0D"},"source":["## Imports, Instalations and Constants"]},{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets"],"metadata":{"id":"skPJ3pBP7JFD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"edc6dd5b-17f4-42dc-eb16-00123f8a3c85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 5.1 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 20.9 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 22.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 1.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n","Collecting datasets\n","  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n","\u001b[K     |████████████████████████████████| 325 kB 5.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 7.6 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 30.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 34.6 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 33.8 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 32.1 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 33.6 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","metadata":{"id":"KXQ-gIvNedL5"},"source":["import pandas as pd\n","import tensorflow as tf\n","import transformers\n","from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForSequenceClassification, EvalPrediction, GlueDataset\n","from transformers import ConvBertTokenizer\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from tensorflow.keras.utils import to_categorical\n","\n","pd.set_option('display.max_colwidth', None)\n","BATCH_SIZE = 16\n","N_EPOCHS = 3 # we can put more, because evaluation of the model shows big difference in loss with accuracy 1.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J57FheO30wA6"},"source":["## A common data set (with source text, preprocesses text, new features, and labels) before text-to-sequence transformation"]},{"cell_type":"markdown","metadata":{"id":"R83M04NVprbX"},"source":["We will take a column with not preprocecced text data for pure experiment with Hugging Face distilbert model"]},{"cell_type":"code","source":["# test = pd.read_csv('drugsComTest_raw.tsv', sep='\\t')\n","# train = pd.read_csv('drugsComTrain_raw.tsv', sep='\\t')\n","# df = pd.concat([train,test])\n","\n","loaded_df = pd.read_csv('drugsComTrain_raw.tsv', sep='\\t')\n","# df = loaded_df[['review', 'rating']]\n","df = loaded_df[:10000]\n","\n","\n","def get_sentiment(rating):\n","  if rating < 4.0:\n","    return 'neg'\n","  elif rating >= 4.0 and rating <= 7.0:\n","    return 'neutral'\n","  else:\n","    return 'pos'\n","\n","df['sentiment'] = df['rating'].map(lambda x: get_sentiment(x))"],"metadata":{"id":"zQmvmiyyq8Py","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a93e5e2-be16-4179-bf25-d34489eceeba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]}]},{"cell_type":"markdown","source":["https://www.sunnyville.ai/fine-tuning-distilbert-multi-class-text-classification-using-transformers-and-tensorflow/  below"],"metadata":{"id":"ofUnyZtQbI06"}},{"cell_type":"code","source":["encode_method = 'onehot'\n","\n","\n","if encode_method == 'encode':\n","    print('encode_method: ', encode_method)\n","    df['encoded_sent'] = df['sentiment'].astype('category').cat.codes\n","\n","    data_texts = df[\"review\"].to_list() # Features (not-tokenized yet)\n","    data_labels = df[\"encoded_sent\"].to_list() # Lables\n","    X_train, X_test, y_train, y_test = train_test_split(data_texts, data_labels, test_size=0.3, random_state=1)\n","\n","\n","    print(X_train[:10])\n","elif encode_method == 'onehot':\n","    print('encode_method: ', encode_method)\n","    # encode class names to integers\n","    labelencoder = preprocessing.LabelEncoder()\n","    labels = labelencoder.fit_transform(df['sentiment'])\n","\n","    cat_labels = to_categorical(labels)\n","\n","    X_train, X_test, y_train, y_test = train_test_split(df['review'], cat_labels, random_state=1)\n","\n","    X_train = X_train.to_list()\n","    X_test = X_test.to_list()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B5wfyxxbbG4B","outputId":"918276fc-7c78-4ed9-d663-2c20b2407330"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["encode_method:  onehot\n"]}]},{"cell_type":"code","source":["# print(labels)\n","# cat_labels.shape\n","# df.head()\n","labelencoder.classes_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RDLzfwS37P1H","outputId":"08c471a8-ba37-48fa-ac21-e23f04f2d759"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['neg', 'neutral', 'pos'], dtype=object)"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":[""],"metadata":{"id":"8kmkdXR97xUw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ConvBERT"],"metadata":{"id":"pBG5esyvv1lA"}},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"3LfF5nRS_lWO"}},{"cell_type":"code","source":["# # EXAMPLE\n","# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')   # switch to MODEL_NAME\n","# train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","# val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n","\n","tokenizer_convbert = ConvBertTokenizer.from_pretrained(\"YituTech/conv-bert-base\")\n","\n","train_encodings_convbert = tokenizer_convbert(X_train, truncation=True, padding=True)\n","test_encodings_convbert = tokenizer_convbert(X_test, truncation=True, padding=True)"],"metadata":{"id":"L6ucVTtCv3x3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset_convbert = tf.data.Dataset.from_tensor_slices((\n","    dict(train_encodings_convbert),\n","    y_train\n","))\n","val_dataset_convbert = tf.data.Dataset.from_tensor_slices((\n","    dict(test_encodings_convbert),\n","    y_test\n",")) "],"metadata":{"id":"q6Ry0LRe0omi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset_convbert"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PePKNjPgSKsW","outputId":"fa195166-3c05-4d29-9ddd-da9b37045523"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<TensorSliceDataset element_spec=({'input_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(512,), dtype=tf.int32, name=None)}, TensorSpec(shape=(3,), dtype=tf.float32, name=None))>"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["from transformers import TFConvBertForSequenceClassification\n","\n","model_convbert = TFConvBertForSequenceClassification.from_pretrained(\"YituTech/conv-bert-base\", problem_type=\"multi_label_classification\", num_labels=3)\n","\n","learning_rate = 5e-8  # 5e-5 = 0.00005\n","optimizer_convbert = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","loss_convbert = tf.keras.losses.CategoricalCrossentropy() # Computes the crossentropy loss between the labels and predictions. \n","model_convbert.compile(optimizer=optimizer_convbert, loss=loss_convbert, metrics=['accuracy'])\n","# model_convbert.compile(optimizer=optimizer_convbert, loss=loss_convbert, metrics=['categorical_accuracy'])\n","\n","# model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRk63l230vcf","outputId":"7c23e8be-61f5-48dc-fe89-7c0384ccc36e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n","\n","Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["len(X_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"no0Wd_3H_RnZ","outputId":"407a55cb-d8a5-4755-f65b-d8efd3a5a6c6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7500"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["BATCH_SIZE = 8\n","\n","# model_convbert.fit(train_dataset_convbert.shuffle(len(X_train)).batch(BATCH_SIZE), \n","#           epochs=N_EPOCHS,\n","#           batch_size=BATCH_SIZE)\n","\n","model_convbert.fit(train_dataset_convbert.batch(BATCH_SIZE), \n","          epochs=N_EPOCHS,\n","          batch_size=BATCH_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5Nxzn_w-mvU","outputId":"1fe25935-e951-4f18-8c05-abc7314e1d99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","938/938 [==============================] - 1822s 2s/step - loss: 2.8387 - accuracy: 0.5549\n","Epoch 2/3\n","938/938 [==============================] - 1785s 2s/step - loss: 2.7105 - accuracy: 0.5540\n","Epoch 3/3\n","938/938 [==============================] - 1790s 2s/step - loss: 2.6075 - accuracy: 0.5536\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fcc8daeb210>"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["## save zip file to my drive"],"metadata":{"id":"BFd7oY1CRzGV"}},{"cell_type":"code","source":["model_save_name = \"04-08-model_convbert_v3\""],"metadata":{"id":"Bbh9T7UHN6Cb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SAVE MODEL  ( https://huggingface.co/docs/transformers/main_classes/model )\n","model_convbert.save_pretrained(model_save_name)\n","!zip -r 04-08-model_convbert_v3.zip 04-08-model_convbert_v3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CpKtqB9QQswf","outputId":"38d38e2f-5355-4add-c5d9-aa264f872cce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: 04-08-model_convbert_v3/ (stored 0%)\n","  adding: 04-08-model_convbert_v3/config.json (deflated 52%)\n","  adding: 04-08-model_convbert_v3/tf_model.h5 (deflated 7%)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# with open('/content/drive/My Drive/NLP Models/foo.txt', 'w') as f:\n","#   f.write('Hello Google Drive!')\n","# !cat /content/drive/My\\ Drive/NLP\\ Models/foo.txt\n","\n","!cp 04-08-model_convbert_v3.zip \"/content/drive/My Drive/04-08-model_convbert_v3.zip\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWeyMEzPOm1V","outputId":"f3238e2c-1dec-46dc-c444-fe25c3427638"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Download to my local computer\n","\n","from google.colab import files\n","files.download('04-08-model_convbert_v3.zip')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"VxzXYCM_Q9qT","outputId":"5695578d-314a-4fc0-f2ad-666b4cd6a67d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_23af4f87-d665-4f21-a8ee-2a4388b580b4\", \"04-08-model_convbert_v3.zip\", 394002416)"]},"metadata":{}}]},{"cell_type":"markdown","source":["## Evaluate loaded model"],"metadata":{"id":"LfCGTmxg_2Tb"}},{"cell_type":"code","source":["loaded_model = TFConvBertForSequenceClassification.from_pretrained(\"04-08-model_convbert_v3\", problem_type=\"multi_label_classification\", num_labels=3)   # switch to MODEL_NAME"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PtcHpEgERAfQ","outputId":"bde2617f-8d8c-4929-94c5-8aed59890e51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n","\n","All the layers of TFConvBertForSequenceClassification were initialized from the model checkpoint at 04-08-model_convbert_v3.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFConvBertForSequenceClassification for predictions without further training.\n"]}]},{"cell_type":"markdown","source":["### with predict_proba function"],"metadata":{"id":"_fihoZNkAA7u"}},{"cell_type":"code","source":["# MAX_LEN = X_train.apply(lambda s: len([x for x in s.split()])).max()\n","\n","MAX_LEN = max([len(x) for x in X_train])\n","MAX_LEN"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ULGl6mx_M9hS","outputId":"af131a6a-ff5a-4ad6-d7ef-9a7f35e76189"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2436"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["def predict_proba(text_list, model, tokenizer):\n","  \"\"\"\n","  To get array with predicted probabilities for 0 - instructions, 1- ingredients classes \n","  for each paragraph in the list of strings\n","  :param text_list: list[str]\n","  :param model: transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification\n","  :param tokenizer: transformers.models.distilbert.tokenization_distilbert.DistilBertTokenizer\n","  :return res: numpy.ndarray\n","  \"\"\"\n","     \n","  encodings = tokenizer(text_list, max_length=MAX_LEN, truncation=True, padding=True)\n","  dataset = tf.data.Dataset.from_tensor_slices((dict(encodings))) \n","  preds = model.predict(dataset.batch(1)).logits\n","  res = tf.nn.softmax(preds, axis=1).numpy()\n","    \n","  return res"],"metadata":{"id":"wsRUENmKMQo_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["string1 = ['This helped a lot.']\n","\n","predict_proba(string1, model_convbert, tokenizer_convbert)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1g9mcTn6L9uz","outputId":"4e07b9c0-24b3-4503-d951-c491b6e27e00"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.32198203, 0.31275803, 0.36525995]], dtype=float32)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["predict_proba(string1, loaded_model, tokenizer_convbert)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZSh_YwMNu-T","outputId":"c93340b9-ba38-415a-8fdc-67c76be34426"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.32198203, 0.31275803, 0.36525995]], dtype=float32)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["# string2 = ['i felt sick after 2 days']\n","string2 = ['this was bad']\n","\n","\n","predict_proba(string2, model_convbert, tokenizer_convbert)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X80cAXplzC7v","outputId":"13720a35-835a-46b7-f33b-f6b5c6bc5ce6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcc15c75200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[0.3211965 , 0.31004646, 0.36875707]], dtype=float32)"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["predict_proba(string2, loaded_model, tokenizer_convbert)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kj8X51MtzH2B","outputId":"e3fd6c33-dab3-4250-ce6c-fb72fb8e5b2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcc178e28c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[0.3211965 , 0.31004646, 0.36875707]], dtype=float32)"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["### with encoded data"],"metadata":{"id":"IXXBlLmfAC-f"}},{"cell_type":"code","source":["# Load test data\n","loaded_test_df = pd.read_csv('drugsComTest_raw.tsv', sep='\\t')\n","test_df = loaded_test_df[:2000]\n","\n","test_df['sentiment'] = test_df['rating'].map(lambda x: get_sentiment(x))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNFl0D-mAXUR","outputId":"11b7f85d-e5e1-4305-d94d-eb3f4f9ddc51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"\n"]}]},{"cell_type":"code","source":["labelencoder = preprocessing.LabelEncoder()\n","labels = labelencoder.fit_transform(test_df['sentiment'])\n","\n","cat_labels = to_categorical(labels)\n","\n","X_test_final = test_df['review']\n","X_test_final = X_test_final.to_list()\n","\n","y_test_final = cat_labels"],"metadata":{"id":"kXMbjvd5Ri40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loaded_test_df.head()\n","X_test_final[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"54yv7PETHznJ","outputId":"fd4bb351-4918-4cf0-f1b8-eae4c4420d58"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\"I&#039;ve tried a few antidepressants over the years (citalopram, fluoxetine, amitriptyline), but none of those helped with my depression, insomnia &amp; anxiety. My doctor suggested and changed me onto 45mg mirtazapine and this medicine has saved my life. Thankfully I have had no side effects especially the most common - weight gain, I&#039;ve actually lost alot of weight. I still have suicidal thoughts but mirtazapine has saved me.\"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":141}]},{"cell_type":"code","source":["# tokenizer_convbert = ConvBertTokenizer.from_pretrained(\"YituTech/conv-bert-base\")\n","\n","final_test_encodings_convbert = tokenizer_convbert(X_test_final, truncation=True, padding=True)\n","final_test_dataset_convbert = tf.data.Dataset.from_tensor_slices((\n","    dict(final_test_encodings_convbert),\n","    y_test_final\n","))\n","\n","final_test_dataset_convbert"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BqY1m0H-AFTd","outputId":"55d78ce2-3247-4231-fe5e-29eafe1e4326"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<TensorSliceDataset element_spec=({'input_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(512,), dtype=tf.int32, name=None)}, TensorSpec(shape=(3,), dtype=tf.float32, name=None))>"]},"metadata":{},"execution_count":144}]},{"cell_type":"code","source":["val_dataset_convbert"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4sf20JFcUjL","outputId":"943839a4-b039-4c08-d013-c69b4090a964"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<TensorSliceDataset element_spec=({'input_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(512,), dtype=tf.int32, name=None)}, TensorSpec(shape=(3,), dtype=tf.float32, name=None))>"]},"metadata":{},"execution_count":145}]},{"cell_type":"code","source":["loaded_model.compile(optimizer_convbert, loss_convbert)"],"metadata":{"id":"JWdBAa6WMvAe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model.metrics_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8SfZVewT2hP","outputId":"574c78cf-99a3-41a8-fcc1-0437da4d32dd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":147}]},{"cell_type":"markdown","source":["info on evaluating model\n","https://swatimeena989.medium.com/bert-text-classification-using-keras-903671e0207d#a1bb"],"metadata":{"id":"NpWperQNt2CM"}},{"cell_type":"markdown","source":["#### evaluate with val data"],"metadata":{"id":"QcblYjq9b_PV"}},{"cell_type":"code","source":["# Get predictions with validation set\n","y_pred = loaded_model.predict(val_dataset_convbert.batch(16))\n","y_pred_proba = [float(x[1]) for x in tf.nn.softmax(y_pred.logits)]\n","y_pred_label = [0 if x[0] > x[1] else 1 for x in tf.nn.softmax(y_pred.logits)]\n","\n"],"metadata":{"id":"fdpTN_sGFhWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"oqP-Ys1nYNJt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.shape(y_pred.logits)\n","pred_labels = np.argmax(y_pred.logits, axis=1)\n","# pred_labels = np.argmax(y_pred, axis=1)\n","pred_labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LKQOsSmAXqw5","outputId":"d8fcc9ad-ab2e-4ebb-ce51-1a535d311de5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 2, 2, ..., 2, 2, 2])"]},"metadata":{},"execution_count":125}]},{"cell_type":"code","source":["y_test_labels = np.argmax(y_test, axis=1)\n","y_test_labels"],"metadata":{"id":"aXnhixywZOID"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report"],"metadata":{"id":"m82VWQRiZp-t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(y_test_labels,pred_labels,target_names=labelencoder.classes_))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MAxEZDliZdhR","outputId":"5b90e5f4-9ee6-4336-cd43-c759be009fe9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","         neg       0.00      0.00      0.00       550\n","     neutral       0.00      0.00      0.00       443\n","         pos       0.60      1.00      0.75      1507\n","\n","    accuracy                           0.60      2500\n","   macro avg       0.20      0.33      0.25      2500\n","weighted avg       0.36      0.60      0.45      2500\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["#### evaluate with test data"],"metadata":{"id":"H6Znk9u_cENH"}},{"cell_type":"code","source":["# Get predictions with validation set\n","y_final_pred = loaded_model.predict(final_test_dataset_convbert.batch(16))\n","y_final_pred_proba = [float(x[1]) for x in tf.nn.softmax(y_final_pred.logits)]\n","y_final_pred_label = [0 if x[0] > x[1] else 1 for x in tf.nn.softmax(y_final_pred.logits)]"],"metadata":{"id":"CmIPDY2dcJIs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.shape(y_final_pred.logits)\n","final_pred_labels = np.argmax(y_final_pred.logits, axis=1)\n","# pred_labels = np.argmax(y_pred, axis=1)\n","final_pred_labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aiUDCizrcK8i","outputId":"40532b72-7943-4ff8-e769-5e7009bc7830"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 2, 2, ..., 2, 2, 2])"]},"metadata":{},"execution_count":150}]},{"cell_type":"code","source":["y_test_final_labels = np.argmax(y_test_final, axis=1)\n","y_test_final_labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1B-HBIzecMrx","outputId":"54edbe16-4ec2-4c67-9f86-a7cf0f63e7cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 2, 2, ..., 2, 0, 2])"]},"metadata":{},"execution_count":151}]},{"cell_type":"code","source":["print(classification_report(y_test_final_labels,final_pred_labels,target_names=labelencoder.classes_))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLKpGfw_cNwr","outputId":"6305d346-8445-4e5b-fde4-2915d8a106a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","         neg       0.00      0.00      0.00       446\n","     neutral       0.00      0.00      0.00       348\n","         pos       0.60      1.00      0.75      1206\n","\n","    accuracy                           0.60      2000\n","   macro avg       0.20      0.33      0.25      2000\n","weighted avg       0.36      0.60      0.45      2000\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["\n","# # Evaluate the model\n","# from sklearn.metrics import (\n","#     confusion_matrix,\n","#     roc_auc_score,\n","#     average_precision_score,\n","# )\n","\n","# # print(\"Confusion Matrix : \")\n","# # print(confusion_matrix(y_test, y_pred_label))\n","\n","# print(\"ROC AUC score : \", round(roc_auc_score(y_test, y_pred_proba), 3))\n","\n","# print(\"Average Precision score : \", round(average_precision_score(y_test, y_pred_proba), 3))\n"],"metadata":{"id":"xl-XrCEhWMnw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_text = X_test_final[0]\n","test_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"OHaFqd6vHuhC","outputId":"d3c80b8b-4b62-43b7-8ffe-508085c5e730"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\"I&#039;ve tried a few antidepressants over the years (citalopram, fluoxetine, amitriptyline), but none of those helped with my depression, insomnia &amp; anxiety. My doctor suggested and changed me onto 45mg mirtazapine and this medicine has saved my life. Thankfully I have had no side effects especially the most common - weight gain, I&#039;ve actually lost alot of weight. I still have suicidal thoughts but mirtazapine has saved me.\"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["predict_input = tokenizer_convbert.encode(test_text,\n","                                 truncation=True,\n","                                 padding=True,\n","                                 return_tensors=\"tf\")\n","\n","output = loaded_model(predict_input)[0]\n","\n","prediction_value = tf.argmax(output, axis=1).numpy()[0]\n","prediction_value"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9-O30JxHpVf","outputId":"27ffbe52-af7c-4202-aa6a-29f6b947d8fa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["# not run after this"],"metadata":{"id":"mOu00nOEM_gL"}},{"cell_type":"code","source":["# https://huggingface.co/docs/transformers/training\n","\n","from datasets import load_metric\n","from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(output_dir=\"training_args_convbert\")\n","metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","\n","training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n","\n","\n","trainer = Trainer(\n","    model=model_convbert,\n","    args=training_args,\n","    train_dataset=X_train,\n","    eval_dataset=X_test,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"7Nl5hJ6u3SMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"voeLR1GY7sAW","outputId":"5270e84c-42e6-40a3-bb3f-a48feca4d2ff","colab":{"base_uri":"https://localhost:8080/","height":511}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 1400\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 525\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1396\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_default_data_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_default_data_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"]}]},{"cell_type":"code","source":["# model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16,\n","#           validation_data=val_dataset.shuffle(1000).batch(16))\n","model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE), \n","          epochs=N_EPOCHS,\n","          batch_size=BATCH_SIZE)"],"metadata":{"id":"ZwkJC0UX0xtV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train[:10]"],"metadata":{"id":"BM8PiD1g_8kS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sklearn import preprocessing\n","# # from sklearn.preprocessing import OneHotEncoder\n","# from tensorflow.keras.utils import to_categorical\n"," \n","# # encode class names to integers\n","# labelencoder = preprocessing.LabelEncoder()\n","# y_train_encode_df = pd.DataFrame(labelencoder.fit_transform(y_train))\n","# y_test_encode_df = pd.DataFrame(labelencoder.fit_transform(y_test))\n"," \n","# y_train = to_categorical(y_train_encode_df)\n","# # ydev = to_categorical(labels_val.values)\n","# y_test = to_categorical(y_test_encode_df)\n","\n","\n","# # encoder = OneHotEncoder(handle_unknown='ignore')\n","# # y_train_encoder_df = pd.DataFrame(encoder.fit_transform(y_train).toarray())\n","# # y_train = df.join(y_train_encoder_df)\n"],"metadata":{"id":"3I0iwvq5-EWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train"],"metadata":{"id":"FPBg8C8K_a7O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test"],"metadata":{"id":"7KVbZbGq7rgg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FyYcUwvZd3nf"},"source":["## check the shapes and split proportion "]},{"cell_type":"code","metadata":{"id":"6jObI-YODXwr"},"source":["X_train.shape, X_test.shape, y_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcKHwsUwfpiF"},"source":["print('The proportion in y_train\\n',y_train.value_counts(normalize=True).mul(100))\n","print('The proportion in y_test\\n',y_test.value_counts(normalize=True).mul(100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_T0xiN5JTNAn"},"source":["## Preprocess"]},{"cell_type":"markdown","metadata":{"id":"5c6psE19Sywd"},"source":["### Decode byte arrays into string representation. "]},{"cell_type":"code","metadata":{"id":"CvxV2NEzgaJd"},"source":["# X_train = X_train.apply(lambda x: str(x[0], 'utf-8'))\n","# X_test = X_test.apply(lambda x:  str(x[0], 'utf-8'))\n","# X_train[:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sent_lens = [len(sent) for sent in X_train]\n","MAX_LEN = max(sent_lens)"],"metadata":{"id":"vg3pFAviRv-V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"au36t-dZFRjh"},"source":["### Max sentence length"]},{"cell_type":"code","source":["# rename variables\n","X_train = train_texts\n","X_test = val_texts\n","y_train = train_labels\n","y_test = val_labels\n","\n","test_dataset = val_dataset"],"metadata":{"id":"p8KNW5vFNtHw"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7eBV0wMcCeB3"},"source":["# MAX_LEN = X_train.apply(lambda s: len([x for x in s.split()])).max()\n","# MAX_LEN"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKZWPmE6T9P8"},"source":["## Encode with  DistilBertTokenizer"]},{"cell_type":"code","metadata":{"id":"OtFwP_Egzktr"},"source":["#define a tokenizer object\n","tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n","\n","#tokenize the text (padding to max sequence in batch)\n","train_encodings = tokenizer(list(X_train.values), truncation=True, padding=True)\n","test_encodings = tokenizer(list(X_test.values), truncation=True, padding=True)\n","\n","#print the first paragraph and it transformation\n","print(f'First paragraph: \\'{X_train[:1]}\\'')\n","print(f'Input ids: {train_encodings[\"input_ids\"][0]}')\n","print(f'Attention mask: {train_encodings[\"attention_mask\"][0]}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CUmtW9dgEw8a"},"source":["## Length check"]},{"cell_type":"code","metadata":{"id":"G-OtFxDUEhHn"},"source":["# pd.DataFrame(train_encodings[\"input_ids\"]).hist();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKWUEbpFEt7N"},"source":["len(train_encodings[\"attention_mask\"][0]) #max len tokenized sentence - 362"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BaJnImyanka0"},"source":["len(train_encodings[\"input_ids\"][0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X8Uy0qycy8Fl"},"source":["###  Turn our labels and encodings into a tf.Dataset object"]},{"cell_type":"code","metadata":{"id":"o5U_xzK8fVrc"},"source":["# train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),\n","#                                                     list(y_train.values)))\n","\n","# test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),\n","#                                                     list(y_test.values)))\n","\n","# y_train and y_test are one-hot encoded arrays\n","train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),\n","                                                    list(y_train)))\n","\n","test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),\n","                                                    list(y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7UuD_KewylTi"},"source":["train_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.data.experimental.cardinality(train_dataset)\n"],"metadata":{"id":"AsaGTeh6XL5T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kg8b3C855T67"},"source":["## Fine-tuning with native TensorFlow\n"]},{"cell_type":"code","metadata":{"id":"hamHq6LO0AiS"},"source":["model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n","\n","optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","# losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # Computes the crossentropy loss between the labels and predictions. \n","losss = tf.keras.losses.CategoricalCrossentropy() # Computes the crossentropy loss between the labels and predictions. \n","model.compile(optimizer=optimizerr,\n","              loss=losss,\n","              metrics=['accuracy'])\n","\n","model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE), \n","          epochs=N_EPOCHS,\n","          batch_size=BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME, problem_type=\"multi_label_classification\")\n","\n","optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","# losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # Computes the crossentropy loss between the labels and predictions. \n","losss = tf.keras.losses.CategoricalCrossentropy() # Computes the crossentropy loss between the labels and predictions. \n","model.compile(optimizer=optimizerr,\n","              loss=losss,\n","              metrics=['accuracy'])"],"metadata":{"id":"rNdTktxfRctt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(train_dataset.shuffle(len(X_train)).batch(BATCH_SIZE), \n","          epochs=N_EPOCHS,\n","          batch_size=BATCH_SIZE)"],"metadata":{"id":"qgzbcPScRfCM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Zv-nDPsUZdF"},"source":["## Model Evaluation"]},{"cell_type":"code","metadata":{"id":"kO3wessE5GKE"},"source":["model.evaluate(test_dataset.shuffle(len(X_test)).batch(BATCH_SIZE), return_dict=True, batch_size=BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1g2yK0uG1sKr"},"source":["## Predict on the different text examples"]},{"cell_type":"code","metadata":{"id":"ncmEZIi-_u7d"},"source":["def predict_proba(text_list, model, tokenizer):\n","  \"\"\"\n","  To get array with predicted probabilities for 0 - instructions, 1- ingredients classes \n","  for each paragraph in the list of strings\n","  :param text_list: list[str]\n","  :param model: transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification\n","  :param tokenizer: transformers.models.distilbert.tokenization_distilbert.DistilBertTokenizer\n","  :return res: numpy.ndarray\n","  \"\"\"\n","     \n","  encodings = tokenizer(text_list, max_length=MAX_LEN, truncation=True, padding=True)\n","  dataset = tf.data.Dataset.from_tensor_slices((dict(encodings))) \n","  preds = model.predict(dataset.batch(1)).logits\n","  res = tf.nn.softmax(preds, axis=1).numpy()\n","    \n","  return res"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xliDBCGFio6"},"source":["We take a txt file [here](https://github.com/Galina-Blokh/ai_assignment_aidock/blob/refator/data/test_links.txt). This file contains links to the recipe pages which our model didn't saw yet. Assuming you scraped data from the first [url](https://www.loveandlemons.com/green-bean-salad-recipe/). The data you feed into your model for prediction will be looking like in the cell below. (*A list with one first string of ingredients and following three strings with instructions.)"]},{"cell_type":"code","metadata":{"id":"BgrcvypsdcZR"},"source":["strings_list =[\"\"\"\n","                  1 pound green beans, trimmed\n","                  ½ head radicchio, sliced into strips\n","                  Scant ¼ cup thinly sliced red onion\n","                  Honey Mustard Dressing, for drizzling\n","                  2 ounces goat cheese\n","                  2 tablespoons chopped walnuts\n","                  2 tablespoons sliced almonds\n","                  ¼ cup tarragon\n","                  Flaky sea salt\n","                  \"\"\",\n","                  \"\"\"\n","                  Bring a large pot of salted water to a boil and set a bowl of ice water nearby.\n","                  Drop the green beans into the boiling water and blanch for 2 minutes.\n","                    Remove the beans and immediately immerse in the ice water long enough \n","                    to cool completely, about 15 seconds. Drain and place on paper towels to dry.\n","                  \"\"\",\n","                  \"\"\"\n","                  Transfer the beans to a bowl and toss with the radicchio, onion, \n","                  and a few spoonfuls of the dressing.\n","                  \"\"\",\n","                  \"\"\"\n","                  Arrange on a platter and top with small dollops of goat cheese, the walnuts, \n","                  almonds, and tarragon. Drizzle with more dressing, season to taste with flaky \n","                  salt, and serve.\n","                  \"\"\"]\n","predict_proba(strings_list, model, tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fR_Q36V1Ok5L"},"source":["The result of the predictive function gives an array of arrays. Each inner array contains probability for 0 and 1 classes (i.e. for instructions and ingredients labels). We got a pretty accurate model!\n","\n","Even if you'll do a single paragraph as an input, you'll get a very accurate model's answer (data from [second line in .txt document](https://github.com/Galina-Blokh/ai_assignment_aidock/blob/refator/data/test_links.txt) - recipe page [url](https://www.loveandlemons.com/any-vegetable-vinegar-pickles/))"]},{"cell_type":"code","metadata":{"id":"QTMpltD21Eju"},"source":["string1 = [\"\"\"\n","            any vegetables you like (I used cucumbers, broccoli, cauliflower, onions and radishes)\n","            fresh or dried spices (I used peppercorns, cumin, coriander, mustard seeds, & caraway)\n","            1 cup any kind of vinegar (I used white wine vinegar)\n","            1 cup filtered water\n","            1 tablespoon kosher or any non-iodized salt\n","            optional: 1 teaspoon sugar\n","            \"\"\"]\n","predict_proba(string1, model, tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m9Ypv-4zNjkv"},"source":["string2 = ['Wash and cut up your vegetables and pack them into a clean jar.']\n","\n","predict_proba(string2, model, tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pagN91JCNjeb"},"source":["string3 = ['Add between ¼ - ½ teaspoon of whole dried spices.']\n","\n","predict_proba(string3, model, tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jX4tro3NjYz"},"source":["string4 = ['Combine vinegar, filtered water and salt in a medium saucepan and bring to a boil.']\n","\n","predict_proba(string4, model, tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WH52Us5NjR6"},"source":["string5 = ['Put your just boiled brine over the vegetables in the jar.']\n","\n","predict_proba(string5, model, tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7z3GvARzNjMQ"},"source":["string6 = ['Wipe any vinegar spills from the rim with a clean towel and put on the lid.']\n","\n","predict_proba(string6, model, tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"guZ2UPN3NjEy"},"source":["string7 = ['Hide the jar in the back of the friedge for at least a week. Two weeks is better, three is best.']\n","\n","predict_proba(string7, model, tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QvdElPGwNi4z"},"source":["string8 = ['Keep them in the fridge for up to 6 months.']\n","\n","predict_proba(string8, model, tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9cED-4GqVO0M"},"source":["## Well, now you know all steps of how to fine-tune the Hugging Face DistilBert model with Tensorflow API"]},{"cell_type":"markdown","metadata":{"id":"3pDWYy3_rEu4"},"source":["## The end"]},{"cell_type":"code","metadata":{"id":"66fULdpyrGVW"},"source":[""],"execution_count":null,"outputs":[]}]}